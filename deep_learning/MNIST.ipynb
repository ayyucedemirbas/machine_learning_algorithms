{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6jw8bsjURul"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcJpkT-OUbFM",
        "outputId": "8a0648cd-d516-400c-f869-9014289f57f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "mnist_data = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist_data.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHNRmgDtUh0z"
      },
      "source": [
        "def scale_mnist_data(train_images, test_images):\n",
        "    \"\"\"\n",
        "    This function takes in the training and test images as loaded in the cell above, and scales them\n",
        "    so that they have minimum and maximum values equal to 0 and 1 respectively.\n",
        "    Your function should return a tuple (train_images, test_images) of scaled training and test images.\n",
        "    \"\"\"\n",
        "    #train_images=train_images/ 255.\n",
        "    #test_images=test_images / 255.\n",
        "    scaled_train_images, scaled_test_images = train_images / 255., test_images / 255.\n",
        "    return scaled_train_images, scaled_test_images\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flAIAat7Uk7M"
      },
      "source": [
        "scaled_train_images, scaled_test_images = scale_mnist_data(train_images, test_images)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgHF4LFhUmCP"
      },
      "source": [
        "scaled_train_images = scaled_train_images[..., np.newaxis]\n",
        "scaled_test_images = scaled_test_images[..., np.newaxis]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAnLYH_jUqdj"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "\n",
        "def get_model(input_shape):\n",
        "    \"\"\"\n",
        "    This function should build a Sequential model according to the above specification. Ensure the \n",
        "    weights are initialised by providing the input_shape argument in the first layer, given by the\n",
        "    function argument.\n",
        "    Your function should return the model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Conv2D(8, (3,3), padding='SAME', activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D(2,2),\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(10, activation='softmax')])\n",
        "    return model\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ5ss4mZUyB3"
      },
      "source": [
        "\n",
        "model = get_model(scaled_train_images[0].shape)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOLsj3OkU2mX"
      },
      "source": [
        "def compile_model(model):\n",
        "    \"\"\"\n",
        "    This function takes in the model returned from your get_model function, and compiles it with an optimiser,\n",
        "    loss function and metric.\n",
        "    Compile the model using the Adam optimiser (with default settings), the cross-entropy loss function and\n",
        "    accuracy as the only metric. \n",
        "    Your function doesn't need to return anything; the model will be compiled in-place.\n",
        "    \"\"\"\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
        "    acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    mae = tf.keras.metrics.MeanAbsoluteError()\n",
        "    model.compile(\n",
        "    #optimizer='sgd', #'adam', 'rmsprop', 'adadelta'\n",
        "    optimizer= opt,\n",
        "    #loss='binary_crossentropy', # 'mean_squared_error', 'categorical_crossentropy'\n",
        "    loss = 'sparse_categorical_crossentropy', #sparse_categorical_crossentropy --> data labels are integers\n",
        "    #metrics=['accuracy', 'mae']\n",
        "    metrics= [acc, mae])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA_elf0EU4-I"
      },
      "source": [
        "compile_model(model)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im_O7PbaU6Ul"
      },
      "source": [
        "def train_model(model, scaled_train_images, train_labels):\n",
        "    \"\"\"\n",
        "    This function should train the model for 5 epochs on the scaled_train_images and train_labels. \n",
        "    Your function should return the training history, as returned by model.fit.\n",
        "    \"\"\"\n",
        "    history = model.fit(scaled_train_images[...,np.newaxis], train_labels, epochs=5, verbose=2)\n",
        "    return history"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08bS7VR8U_Ox",
        "outputId": "599a3ca2-7b6c-441f-8bcc-c245952e97bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "history = train_model(model, scaled_train_images, train_labels)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 - 19s - loss: 0.1595 - sparse_categorical_accuracy: 0.9512 - mean_absolute_error: 4.3737\n",
            "Epoch 2/5\n",
            "1875/1875 - 19s - loss: 0.0702 - sparse_categorical_accuracy: 0.9787 - mean_absolute_error: 4.3737\n",
            "Epoch 3/5\n",
            "1875/1875 - 19s - loss: 0.0522 - sparse_categorical_accuracy: 0.9839 - mean_absolute_error: 4.3737\n",
            "Epoch 4/5\n",
            "1875/1875 - 19s - loss: 0.0405 - sparse_categorical_accuracy: 0.9876 - mean_absolute_error: 4.3737\n",
            "Epoch 5/5\n",
            "1875/1875 - 20s - loss: 0.0358 - sparse_categorical_accuracy: 0.9895 - mean_absolute_error: 4.3737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-3Je94sdHkG"
      },
      "source": [
        "frame = pd.DataFrame(history.history)"
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}