{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pix2Pix_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUrod784-CLD"
      },
      "source": [
        "Codes are from https://machinelearningmastery.com/how-to-develop-a-pix2pix-gan-for-image-to-image-translation/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uj-1LyJ95c1",
        "outputId": "c59f8542-31f1-4e26-bc98-7610e8663a44"
      },
      "source": [
        "!wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/maps.tar.gz\n",
        "!tar -xf maps.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-21 00:49:21--  http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/maps.tar.gz\n",
            "Resolving efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)... 128.32.244.190\n",
            "Connecting to efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)|128.32.244.190|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 250242400 (239M) [application/x-gzip]\n",
            "Saving to: ‘maps.tar.gz’\n",
            "\n",
            "maps.tar.gz         100%[===================>] 238.65M  3.76MB/s    in 73s     \n",
            "\n",
            "2021-11-21 00:50:34 (3.28 MB/s) - ‘maps.tar.gz’ saved [250242400/250242400]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxYhMesy91fe",
        "outputId": "c1839d7b-38d8-4807-ac66-d1e2d25d911d"
      },
      "source": [
        "from os import listdir\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from matplotlib import pyplot\n",
        "from numpy import zeros, ones, savez_compressed, load, vstack, asarray\n",
        "from numpy.random import randint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, LeakyReLU, Activation, Concatenate, Dropout, BatchNormalization\n",
        " \n",
        "def load_data(path, size=(256,512)):\n",
        "\tsource_list, target_list = list(), list()\n",
        "\t# enumerate filenames in directory, assume all are images\n",
        "\tfor filename in listdir(path):\n",
        "\t\t# load and resize the image\n",
        "\t\tpixels = load_img(path + filename, target_size=size)\n",
        "\t\tpixels = img_to_array(pixels)\n",
        "\t\tsat_img, map_img = pixels[:, :256], pixels[:, 256:]\n",
        "\t\tsource_list.append(sat_img)\n",
        "\t\ttarget_list.append(map_img)\n",
        "\treturn [asarray(source_list), asarray(target_list)]\n",
        " \n",
        "path = 'maps/train/'\n",
        "[source_images, target_images] = load_data(path)\n",
        "print('Loaded: ', source_images.shape, target_images.shape)\n",
        "# save as compressed numpy array\n",
        "filename = 'maps_256.npz'\n",
        "savez_compressed(filename, source_images, target_images)\n",
        "print('Saved dataset: ', filename)\n",
        "n_samples = 3\n",
        "\n",
        "def discriminator(image_shape):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# source image input\n",
        "\tin_src_image = Input(shape=image_shape)\n",
        "\t# target image input\n",
        "\tin_target_image = Input(shape=image_shape)\n",
        "\t# concatenate images channel-wise\n",
        "\tmerged = Concatenate()([in_src_image, in_target_image])\n",
        "\t# C64\n",
        "\tx = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
        "\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\t# C128\n",
        "\tx = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(x)\n",
        "\tx = BatchNormalization()(x)\n",
        "\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\t# C256\n",
        "\tx = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(x)\n",
        "\tx = BatchNormalization()(x)\n",
        "\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\t# C512\n",
        "\tx = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(x)\n",
        "\tx = BatchNormalization()(x)\n",
        "\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\t# second last output layer\n",
        "\tx = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(x)\n",
        "\tx = BatchNormalization()(x)\n",
        "\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\t# patch output\n",
        "\tx = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(x)\n",
        "\tpatch_out = Activation('sigmoid')(x)\n",
        "\t# define model\n",
        "\tmodel = Model([in_src_image, in_target_image], patch_out)\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
        "\treturn model\n",
        "\n",
        "# define an encoder block\n",
        "def encoder(layer_in, n_filters, batchnorm=True):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# add downsampling layer\n",
        "\tx = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
        "\t# conditionally add batch normalization\n",
        "\tif batchnorm:\n",
        "\t\tx = BatchNormalization()(x, training=True)\n",
        "\t# leaky relu activation\n",
        "\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\treturn x\n",
        "\n",
        "# define a decoder block\n",
        "def decoder(layer_in, skip_in, n_filters, dropout=True):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# add upsampling layer\n",
        "\tx = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
        "\t# add batch normalization\n",
        "\tx = BatchNormalization()(x, training=True)\n",
        "\t# conditionally add dropout\n",
        "\tif dropout:\n",
        "\t\tx = Dropout(0.5)(x, training=True)\n",
        "\t# merge with skip connection\n",
        "\tx = Concatenate()([x, skip_in])\n",
        "\t# relu activation\n",
        "\tx = Activation('relu')(x)\n",
        "\treturn x\n",
        "\n",
        "# define the standalone generator model\n",
        "def generator(image_shape=(256,256,3)):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# image input\n",
        "\tin_image = Input(shape=image_shape)\n",
        "\t# encoder model\n",
        "\te1 = encoder(in_image, 64, batchnorm=False)\n",
        "\te2 = encoder(e1, 128)\n",
        "\te3 = encoder(e2, 256)\n",
        "\te4 = encoder(e3, 512)\n",
        "\te5 = encoder(e4, 512)\n",
        "\te6 = encoder(e5, 512)\n",
        "\te7 = encoder(e6, 512)\n",
        "\t# bottleneck, no batch norm and relu\n",
        "\tb = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
        "\tb = Activation('relu')(b)\n",
        "\t# decoder model\n",
        "\td1 = decoder(b, e7, 512)\n",
        "\td2 = decoder(d1, e6, 512)\n",
        "\td3 = decoder(d2, e5, 512)\n",
        "\td4 = decoder(d3, e4, 512, dropout=False)\n",
        "\td5 = decoder(d4, e3, 256, dropout=False)\n",
        "\td6 = decoder(d5, e2, 128, dropout=False)\n",
        "\td7 = decoder(d6, e1, 64, dropout=False)\n",
        "\t# output\n",
        "\tg = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
        "\tout_image = Activation('tanh')(g)\n",
        "\t# define model\n",
        "\tmodel = Model(in_image, out_image)\n",
        "\treturn model\n",
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def gan(g_model, d_model, image_shape):\n",
        "\t# make weights in the discriminator not trainable\n",
        "\tfor layer in d_model.layers:\n",
        "\t\tif not isinstance(layer, BatchNormalization):\n",
        "\t\t\tlayer.trainable = False\n",
        "\t# define the source image\n",
        "\tin_src = Input(shape=image_shape)\n",
        "\t# connect the source image to the generator input\n",
        "\tgen_out = g_model(in_src)\n",
        "\t# connect the source input and generator output to the discriminator input\n",
        "\tdis_out = d_model([in_src, gen_out])\n",
        "\t# src image as input, generated image and classification output\n",
        "\tmodel = Model(in_src, [dis_out, gen_out])\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
        "\treturn model\n",
        "# load and prepare training images\n",
        "def load_real_samples(filename):\n",
        "\t# load compressed arrays\n",
        "\tdata = load(filename)\n",
        "\t# unpack arrays\n",
        "\tX1, X2 = data['arr_0'], data['arr_1']\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\tX1 = (X1 - 127.5) / 127.5\n",
        "\tX2 = (X2 - 127.5) / 127.5\n",
        "\treturn [X1, X2]\n",
        "# select a batch of random samples, returns images and target\n",
        "def generate_real_samples(dataset, n_samples, patch_shape):\n",
        "\t# unpack dataset\n",
        "\ttrainA, trainB = dataset\n",
        "\t# choose random instances\n",
        "\tix = randint(0, trainA.shape[0], n_samples)\n",
        "\t# retrieve selected images\n",
        "\tX1, X2 = trainA[ix], trainB[ix]\n",
        "\t# generate 'real' class labels (1)\n",
        "\ty = ones((n_samples, patch_shape, patch_shape, 1))\n",
        "\treturn [X1, X2], y\n",
        "# generate a batch of images, returns images and targets\n",
        "def generate_fake_samples(g_model, samples, patch_shape):\n",
        "\t# generate fake instance\n",
        "\tX = g_model.predict(samples)\n",
        "\t# create 'fake' class labels (0)\n",
        "\ty = zeros((len(X), patch_shape, patch_shape, 1))\n",
        "\treturn X, y\n",
        "\n",
        "# generate samples and save as a plot and save the model\n",
        "def summarize_performance(step, g_model, dataset, n_samples=3):\n",
        "\t# select a sample of input images\n",
        "\t[X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n",
        "\t# generate a batch of fake samples\n",
        "\tX_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
        "\t# scale all pixels from [-1,1] to [0,1]\n",
        "\tX_realA = (X_realA + 1) / 2.0\n",
        "\tX_realB = (X_realB + 1) / 2.0\n",
        "\tX_fakeB = (X_fakeB + 1) / 2.0\n",
        "\t# plot real source images\n",
        "\tfor i in range(n_samples):\n",
        "\t\tpyplot.subplot(3, n_samples, 1 + i)\n",
        "\t\tpyplot.axis('off')\n",
        "\t\tpyplot.imshow(X_realA[i])\n",
        "\t# plot generated target image\n",
        "\tfor i in range(n_samples):\n",
        "\t\tpyplot.subplot(3, n_samples, 1 + n_samples + i)\n",
        "\t\tpyplot.axis('off')\n",
        "\t\tpyplot.imshow(X_fakeB[i])\n",
        "\t# plot real target image\n",
        "\tfor i in range(n_samples):\n",
        "\t\tpyplot.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
        "\t\tpyplot.axis('off')\n",
        "\t\tpyplot.imshow(X_realB[i])\n",
        "\t# save plot to file\n",
        "\tfilename1 = 'plot_%06d.png' % (step+1)\n",
        "\tpyplot.savefig(filename1)\n",
        "\tpyplot.close()\n",
        "\t# save the generator model\n",
        "\tfilename2 = 'model_%06d.h5' % (step+1)\n",
        "\tg_model.save(filename2)\n",
        "\tprint('>Saved: %s and %s' % (filename1, filename2))\n",
        "\n",
        "# train pix2pix model\n",
        "def train(d_model, g_model, gan_model, dataset, n_epochs=100, n_batch=1):\n",
        "\t# determine the output square shape of the discriminator\n",
        "\tn_patch = d_model.output_shape[1]\n",
        "\t# unpack dataset\n",
        "\ttrainA, trainB = dataset\n",
        "\t# calculate the number of batches per training epoch\n",
        "\tbat_per_epo = int(len(trainA) / n_batch)\n",
        "\t# calculate the number of training iterations\n",
        "\tn_steps = bat_per_epo * n_epochs\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_steps):\n",
        "\t\t# select a batch of real samples\n",
        "\t\t[X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
        "\t\t# generate a batch of fake samples\n",
        "\t\tX_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
        "\t\t# update discriminator for real samples\n",
        "\t\td_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
        "\t\t# update discriminator for generated samples\n",
        "\t\td_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
        "\t\t# update the generator\n",
        "\t\tg_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
        "\t\t# summarize performance\n",
        "\t\tprint('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
        "\t\t# summarize model performance\n",
        "\t\tif (i+1) % (bat_per_epo * 10) == 0:\n",
        "\t\t\tsummarize_performance(i, g_model, dataset)\n",
        "\n",
        "dataset = load_real_samples('maps_256.npz')\n",
        "print('Loaded', dataset[0].shape, dataset[1].shape)\n",
        "# define input shape based on the loaded dataset\n",
        "image_shape = dataset[0].shape[1:]\n",
        "# define the models\n",
        "d_model = discriminator(image_shape)\n",
        "g_model = generator(image_shape)\n",
        "# define the composite model\n",
        "gan_model = gan(g_model, d_model, image_shape)\n",
        "\n",
        "train(d_model, g_model, gan_model, dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded:  (1096, 256, 256, 3) (1096, 256, 256, 3)\n",
            "Saved dataset:  maps_256.npz\n",
            "Loaded (1096, 256, 256, 3) (1096, 256, 256, 3)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    }
  ]
}