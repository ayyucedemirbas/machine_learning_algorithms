{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Codes are from: https://github.com/OlgaChernytska/word2vec-pytorch"
      ],
      "metadata": {
        "id": "dhOOu6gZXJe9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNnqBJ1KTiFT",
        "outputId": "e8632b33-8d35-44e5-88e4-79fc6f90891b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nb-black\n",
            "  Downloading nb_black-1.0.7.tar.gz (4.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from nb-black) (5.5.0)\n",
            "Collecting black>='19.3'\n",
            "  Downloading black-22.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting pathspec>=0.9.0\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting typed-ast>=1.4.2\n",
            "  Downloading typed_ast-1.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 31.1 MB/s \n",
            "\u001b[?25hCollecting platformdirs>=2\n",
            "  Downloading platformdirs-2.5.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from black>='19.3'->nb-black) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from black>='19.3'->nb-black) (3.10.0.2)\n",
            "Collecting click>=8.0.0\n",
            "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click>=8.0.0->black>='19.3'->nb-black) (4.11.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click>=8.0.0->black>='19.3'->nb-black) (3.7.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->nb-black) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->nb-black) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->nb-black) (57.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->nb-black) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->nb-black) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->nb-black) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->nb-black) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->nb-black) (1.0.18)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->nb-black) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->nb-black) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->nb-black) (0.7.0)\n",
            "Building wheels for collected packages: nb-black\n",
            "  Building wheel for nb-black (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nb-black: filename=nb_black-1.0.7-py3-none-any.whl size=5297 sha256=f67e0e8079e168a06d1eaf4b9b40acf68be851d12f7dc04f185ea282d8b940e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/b2/88/51c66d23ea5fd0d40ed50997555e15d981d92671376a9a412a\n",
            "Successfully built nb-black\n",
            "Installing collected packages: typed-ast, platformdirs, pathspec, mypy-extensions, click, black, nb-black\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.0.4 which is incompatible.\u001b[0m\n",
            "Successfully installed black-22.1.0 click-8.0.4 mypy-extensions-0.4.3 nb-black-1.0.7 pathspec-0.9.0 platformdirs-2.5.1 typed-ast-1.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install nb-black"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.yaml\n",
        "\n",
        "model_name: cbow\n",
        "\n",
        "dataset: WikiText2\n",
        "data_dir: data/\n",
        "train_batch_size: 96\n",
        "val_batch_size: 96\n",
        "shuffle: True\n",
        "\n",
        "optimizer: Adam\n",
        "learning_rate: 0.025\n",
        "epochs: 5\n",
        "train_steps: \n",
        "val_steps: \n",
        "\n",
        "checkpoint_frequency: \n",
        "model_dir: weights/cbow_WikiText2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK_iV4dNUY-3",
        "outputId": "3ac3d63c-09f1-4c13-e82f-d4ad1385dd8c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "33uCXAScVLo4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial"
      ],
      "metadata": {
        "id": "rYFNaMGSVs9l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CBOW_N_WORDS = 4\n",
        "SKIPGRAM_N_WORDS = 4\n",
        "\n",
        "MIN_WORD_FREQUENCY = 50\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "\n",
        "EMBED_DIMENSION = 300\n",
        "EMBED_MAX_NORM = 1"
      ],
      "metadata": {
        "id": "J__MKcqrVfic"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import WikiText2, WikiText103\n",
        "\n",
        "def get_english_tokenizer():\n",
        "    \"\"\"\n",
        "    Documentation:\n",
        "    https://pytorch.org/text/stable/_modules/torchtext/data/utils.html#get_tokenizer\n",
        "    \"\"\"\n",
        "    tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def get_data_iterator(ds_name, ds_type, data_dir):\n",
        "    if ds_name == \"WikiText2\":\n",
        "        data_iter = WikiText2(root=data_dir, split=(ds_type))\n",
        "    elif ds_name == \"WikiText103\":\n",
        "        data_iter = WikiText103(root=data_dir, split=(ds_type))\n",
        "    else:\n",
        "        raise ValueError(\"Choose dataset from: WikiText2, WikiText103\")\n",
        "    data_iter = to_map_style_dataset(data_iter)\n",
        "    return data_iter\n",
        "\n",
        "\n",
        "def build_vocab(data_iter, tokenizer):\n",
        "    \"\"\"Builds vocabulary from iterator\"\"\"\n",
        "    \n",
        "    vocab = build_vocab_from_iterator(\n",
        "        map(tokenizer, data_iter),\n",
        "        specials=[\"<unk>\"],\n",
        "        min_freq=MIN_WORD_FREQUENCY,\n",
        "    )\n",
        "    vocab.set_default_index(vocab[\"<unk>\"])\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def collate_cbow(batch, text_pipeline):\n",
        "    \"\"\"\n",
        "    Collate_fn for CBOW model to be used with Dataloader.\n",
        "    `batch` is expected to be list of text paragrahs.\n",
        "    \n",
        "    Context is represented as N=CBOW_N_WORDS past words \n",
        "    and N=CBOW_N_WORDS future words.\n",
        "    \n",
        "    Long paragraphs will be truncated to contain\n",
        "    no more that MAX_SEQUENCE_LENGTH tokens.\n",
        "    \n",
        "    Each element in `batch_input` is N=CBOW_N_WORDS*2 context words.\n",
        "    Each element in `batch_output` is a middle word.\n",
        "    \"\"\"\n",
        "    batch_input, batch_output = [], []\n",
        "    for text in batch:\n",
        "        text_tokens_ids = text_pipeline(text)\n",
        "\n",
        "        if len(text_tokens_ids) < CBOW_N_WORDS * 2 + 1:\n",
        "            continue\n",
        "\n",
        "        if MAX_SEQUENCE_LENGTH:\n",
        "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
        "\n",
        "        for idx in range(len(text_tokens_ids) - CBOW_N_WORDS * 2):\n",
        "            token_id_sequence = text_tokens_ids[idx : (idx + CBOW_N_WORDS * 2 + 1)]\n",
        "            output = token_id_sequence.pop(CBOW_N_WORDS)\n",
        "            input_ = token_id_sequence\n",
        "            batch_input.append(input_)\n",
        "            batch_output.append(output)\n",
        "\n",
        "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
        "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
        "    return batch_input, batch_output\n",
        "\n",
        "\n",
        "def collate_skipgram(batch, text_pipeline):\n",
        "    \"\"\"\n",
        "    Collate_fn for Skip-Gram model to be used with Dataloader.\n",
        "    `batch` is expected to be list of text paragrahs.\n",
        "    \n",
        "    Context is represented as N=SKIPGRAM_N_WORDS past words \n",
        "    and N=SKIPGRAM_N_WORDS future words.\n",
        "    \n",
        "    Long paragraphs will be truncated to contain\n",
        "    no more that MAX_SEQUENCE_LENGTH tokens.\n",
        "    \n",
        "    Each element in `batch_input` is a middle word.\n",
        "    Each element in `batch_output` is a context word.\n",
        "    \"\"\"\n",
        "    batch_input, batch_output = [], []\n",
        "    for text in batch:\n",
        "        text_tokens_ids = text_pipeline(text)\n",
        "\n",
        "        if len(text_tokens_ids) < SKIPGRAM_N_WORDS * 2 + 1:\n",
        "            continue\n",
        "\n",
        "        if MAX_SEQUENCE_LENGTH:\n",
        "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
        "\n",
        "        for idx in range(len(text_tokens_ids) - SKIPGRAM_N_WORDS * 2):\n",
        "            token_id_sequence = text_tokens_ids[idx : (idx + SKIPGRAM_N_WORDS * 2 + 1)]\n",
        "            input_ = token_id_sequence.pop(SKIPGRAM_N_WORDS)\n",
        "            outputs = token_id_sequence\n",
        "\n",
        "            for output in outputs:\n",
        "                batch_input.append(input_)\n",
        "                batch_output.append(output)\n",
        "\n",
        "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
        "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
        "    return batch_input, batch_output\n",
        "\n",
        "\n",
        "def get_dataloader_and_vocab(\n",
        "    model_name, ds_name, ds_type, data_dir, batch_size, shuffle, vocab=None\n",
        "):\n",
        "\n",
        "    data_iter = get_data_iterator(ds_name, ds_type, data_dir)\n",
        "    tokenizer = get_english_tokenizer()\n",
        "\n",
        "    if not vocab:\n",
        "        vocab = build_vocab(data_iter, tokenizer)\n",
        "        \n",
        "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "\n",
        "    if model_name == \"cbow\":\n",
        "        collate_fn = collate_cbow\n",
        "    elif model_name == \"skipgram\":\n",
        "        collate_fn = collate_skipgram\n",
        "    else:\n",
        "        raise ValueError(\"Choose model from: cbow, skipgram\")\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        data_iter,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=partial(collate_fn, text_pipeline=text_pipeline),\n",
        "    )\n",
        "    return dataloader, vocab\n",
        "    "
      ],
      "metadata": {
        "id": "05K582K9Vytz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CBOW_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of CBOW model described in paper:\n",
        "    https://arxiv.org/abs/1301.3781\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int):\n",
        "        super(CBOW_Model, self).__init__()\n",
        "        self.embeddings = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=EMBED_DIMENSION,\n",
        "            max_norm=EMBED_MAX_NORM,\n",
        "        )\n",
        "        self.linear = nn.Linear(\n",
        "            in_features=EMBED_DIMENSION,\n",
        "            out_features=vocab_size,\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs_):\n",
        "        x = self.embeddings(inputs_)\n",
        "        x = x.mean(axis=1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SkipGram_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of Skip-Gram model described in paper:\n",
        "    https://arxiv.org/abs/1301.3781\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int):\n",
        "        super(SkipGram_Model, self).__init__()\n",
        "        self.embeddings = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=EMBED_DIMENSION,\n",
        "            max_norm=EMBED_MAX_NORM,\n",
        "        )\n",
        "        self.linear = nn.Linear(\n",
        "            in_features=EMBED_DIMENSION,\n",
        "            out_features=vocab_size,\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs_):\n",
        "        x = self.embeddings(inputs_)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "_hxd9l1_V7E2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "\n",
        "def get_model_class(model_name: str):\n",
        "    if model_name == \"cbow\":\n",
        "        return CBOW_Model\n",
        "    elif model_name == \"skipgram\":\n",
        "        return SkipGram_Model\n",
        "    else:\n",
        "        raise ValueError(\"Choose model_name from: cbow, skipgram\")\n",
        "        return\n",
        "\n",
        "\n",
        "def get_optimizer_class(name: str):\n",
        "    if name == \"Adam\":\n",
        "        return optim.Adam\n",
        "    else:\n",
        "        raise ValueError(\"Choose optimizer from: Adam\")\n",
        "        return\n",
        "    \n",
        "\n",
        "def get_lr_scheduler(optimizer, total_epochs: int, verbose: bool = True):\n",
        "    \"\"\"\n",
        "    Scheduler to linearly decrease learning rate, \n",
        "    so thatlearning rate after the last epoch is 0.\n",
        "    \"\"\"\n",
        "    lr_lambda = lambda epoch: (total_epochs - epoch) / total_epochs\n",
        "    lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda, verbose=verbose)\n",
        "    return lr_scheduler\n",
        "\n",
        "\n",
        "def save_config(config: dict, model_dir: str):\n",
        "    \"\"\"Save config file to `model_dir` directory\"\"\"\n",
        "    config_path = os.path.join(model_dir, \"config.yaml\")\n",
        "    with open(config_path, \"w\") as stream:\n",
        "        yaml.dump(config, stream)\n",
        "        \n",
        "        \n",
        "def save_vocab(vocab, model_dir: str):\n",
        "    \"\"\"Save vocab file to `model_dir` directory\"\"\"\n",
        "    vocab_path = os.path.join(model_dir, \"vocab.pt\")\n",
        "    torch.save(vocab, vocab_path)"
      ],
      "metadata": {
        "id": "N2OTaNyEWHQp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"Main class for model training\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        epochs,\n",
        "        train_dataloader,\n",
        "        train_steps,\n",
        "        val_dataloader,\n",
        "        val_steps,\n",
        "        checkpoint_frequency,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        lr_scheduler,\n",
        "        device,\n",
        "        model_dir,\n",
        "        model_name,\n",
        "    ):  \n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.train_steps = train_steps\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.val_steps = val_steps\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.checkpoint_frequency = checkpoint_frequency\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "        self.device = device\n",
        "        self.model_dir = model_dir\n",
        "        self.model_name = model_name\n",
        "\n",
        "        self.loss = {\"train\": [], \"val\": []}\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.epochs):\n",
        "            self._train_epoch()\n",
        "            self._validate_epoch()\n",
        "            print(\n",
        "                \"Epoch: {}/{}, Train Loss={:.5f}, Val Loss={:.5f}\".format(\n",
        "                    epoch + 1,\n",
        "                    self.epochs,\n",
        "                    self.loss[\"train\"][-1],\n",
        "                    self.loss[\"val\"][-1],\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.lr_scheduler.step()\n",
        "\n",
        "            if self.checkpoint_frequency:\n",
        "                self._save_checkpoint(epoch)\n",
        "\n",
        "    def _train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = []\n",
        "\n",
        "        for i, batch_data in enumerate(self.train_dataloader, 1):\n",
        "            inputs = batch_data[0].to(self.device)\n",
        "            labels = batch_data[1].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "            if i == self.train_steps:\n",
        "                break\n",
        "\n",
        "        epoch_loss = np.mean(running_loss)\n",
        "        self.loss[\"train\"].append(epoch_loss)\n",
        "\n",
        "    def _validate_epoch(self):\n",
        "        self.model.eval()\n",
        "        running_loss = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch_data in enumerate(self.val_dataloader, 1):\n",
        "                inputs = batch_data[0].to(self.device)\n",
        "                labels = batch_data[1].to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                running_loss.append(loss.item())\n",
        "\n",
        "                if i == self.val_steps:\n",
        "                    break\n",
        "\n",
        "        epoch_loss = np.mean(running_loss)\n",
        "        self.loss[\"val\"].append(epoch_loss)\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        \"\"\"Save model checkpoint to `self.model_dir` directory\"\"\"\n",
        "        epoch_num = epoch + 1\n",
        "        if epoch_num % self.checkpoint_frequency == 0:\n",
        "            model_path = \"checkpoint_{}.pt\".format(str(epoch_num).zfill(3))\n",
        "            model_path = os.path.join(self.model_dir, model_path)\n",
        "            torch.save(self.model, model_path)\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save final model to `self.model_dir` directory\"\"\"\n",
        "        model_path = os.path.join(self.model_dir, \"model.pt\")\n",
        "        torch.save(self.model, model_path)\n",
        "\n",
        "    def save_loss(self):\n",
        "        \"\"\"Save train/val loss as json file to `self.model_dir` directory\"\"\"\n",
        "        loss_path = os.path.join(self.model_dir, \"loss.json\")\n",
        "        with open(loss_path, \"w\") as fp:\n",
        "            json.dump(self.loss, fp)"
      ],
      "metadata": {
        "id": "xJTqtS8ZWXDD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(config):\n",
        "    os.makedirs(config[\"model_dir\"])\n",
        "    \n",
        "    train_dataloader, vocab = get_dataloader_and_vocab(\n",
        "        model_name=config[\"model_name\"],\n",
        "        ds_name=config[\"dataset\"],\n",
        "        ds_type=\"train\",\n",
        "        data_dir=config[\"data_dir\"],\n",
        "        batch_size=config[\"train_batch_size\"],\n",
        "        shuffle=config[\"shuffle\"],\n",
        "        vocab=None,\n",
        "    )\n",
        "\n",
        "    val_dataloader, _ = get_dataloader_and_vocab(\n",
        "        model_name=config[\"model_name\"],\n",
        "        ds_name=config[\"dataset\"],\n",
        "        ds_type=\"valid\",\n",
        "        data_dir=config[\"data_dir\"],\n",
        "        batch_size=config[\"val_batch_size\"],\n",
        "        shuffle=config[\"shuffle\"],\n",
        "        vocab=vocab,\n",
        "    )\n",
        "\n",
        "    vocab_size = len(vocab.get_stoi())\n",
        "    print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "    model_class = get_model_class(config[\"model_name\"])\n",
        "    model = model_class(vocab_size=vocab_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer_class = get_optimizer_class(config[\"optimizer\"])\n",
        "    optimizer = optimizer_class(model.parameters(), lr=config[\"learning_rate\"])\n",
        "    lr_scheduler = get_lr_scheduler(optimizer, config[\"epochs\"], verbose=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        epochs=config[\"epochs\"],\n",
        "        train_dataloader=train_dataloader,\n",
        "        train_steps=config[\"train_steps\"],\n",
        "        val_dataloader=val_dataloader,\n",
        "        val_steps=config[\"val_steps\"],\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        checkpoint_frequency=config[\"checkpoint_frequency\"],\n",
        "        lr_scheduler=lr_scheduler,\n",
        "        device=device,\n",
        "        model_dir=config[\"model_dir\"],\n",
        "        model_name=config[\"model_name\"],\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    trainer.save_model()\n",
        "    trainer.save_loss()\n",
        "    save_vocab(vocab, config[\"model_dir\"])\n",
        "    save_config(config, config[\"model_dir\"])\n",
        "    print(\"Model artifacts saved to folder:\", config[\"model_dir\"])\n",
        "    "
      ],
      "metadata": {
        "id": "vflQgM0lUrxv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "     \n",
        "    with open('config.yaml', 'r') as stream:\n",
        "        config = yaml.safe_load(stream)\n",
        "    train(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vK5BjinqVQK6",
        "outputId": "209d1721-38bf-4bdd-cca5-affcf189c7db"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.48M/4.48M [00:00<00:00, 19.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 4099\n",
            "Adjusting learning rate of group 0 to 2.5000e-02.\n",
            "Epoch: 1/5, Train Loss=5.34832, Val Loss=5.08055\n",
            "Adjusting learning rate of group 0 to 2.0000e-02.\n",
            "Epoch: 2/5, Train Loss=5.01511, Val Loss=4.95379\n",
            "Adjusting learning rate of group 0 to 1.5000e-02.\n",
            "Epoch: 3/5, Train Loss=4.89140, Val Loss=4.85704\n",
            "Adjusting learning rate of group 0 to 1.0000e-02.\n",
            "Epoch: 4/5, Train Loss=4.78836, Val Loss=4.78800\n",
            "Adjusting learning rate of group 0 to 5.0000e-03.\n",
            "Epoch: 5/5, Train Loss=4.67909, Val Loss=4.70226\n",
            "Adjusting learning rate of group 0 to 0.0000e+00.\n",
            "Training finished.\n",
            "Model artifacts saved to folder: weights/cbow_WikiText2\n"
          ]
        }
      ]
    }
  ]
}