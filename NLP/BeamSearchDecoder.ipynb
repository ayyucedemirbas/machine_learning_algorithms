{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BeamSearchDecoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpD5wd9ZnoTw",
        "outputId": "e5cdbfce-b68b-4b9a-9e3b-81d9aa054bfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons==0.11.2\n",
            "  Downloading tensorflow_addons-0.11.2-cp37-cp37m-manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 34.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 40.6 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 44.8 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 36.0 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61 kB 38.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 153 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 184 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 215 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 245 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 256 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 286 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 307 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 317 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 337 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 348 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 368 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 399 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 409 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 430 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 440 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 460 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 471 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 491 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 501 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 512 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 522 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 532 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 542 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 563 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 573 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 583 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 593 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 604 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 614 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 624 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 634 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 645 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 665 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 675 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 686 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 696 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 706 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 716 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 727 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 737 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 747 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 768 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 778 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 788 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 798 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 808 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 819 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 829 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 839 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 849 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 860 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 870 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 880 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 890 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 901 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 911 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 921 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 942 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 952 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 962 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 972 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 983 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 993 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0 MB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.0 MB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1 MB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.1 MB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1 MB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 29.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons==0.11.2) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.11.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons==0.11.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_tPQR-8nxnn",
        "outputId": "735a8219-3cbb-4a51-ca43-d4fb3e351b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_nmt():\n",
        "    path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "    path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
        "    return path_to_file"
      ],
      "metadata": {
        "id": "JR4TCOHnnzz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NMTDataset:\n",
        "    def __init__(self, problem_type='en-spa'):\n",
        "        self.problem_type = 'en-spa'\n",
        "        self.inp_lang_tokenizer = None\n",
        "        self.targ_lang_tokenizer = None\n",
        "\n",
        "\n",
        "    def unicode_to_ascii(self, s):\n",
        "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "    ## Step 1 and Step 2 \n",
        "    def preprocess_sentence(self, w):\n",
        "        w = self.unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "        # creating a space between a word and the punctuation following it\n",
        "        # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "        w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "        w = w.strip()\n",
        "\n",
        "        # adding a start and an end token to the sentence\n",
        "        # so that the model know when to start and stop predicting.\n",
        "        w = '<start> ' + w + ' <end>'\n",
        "        return w\n",
        "\n",
        "    def create_dataset(self, path, num_examples):\n",
        "        # path : path to spa-eng.txt file\n",
        "        # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\n",
        "        lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "        word_pairs = [[self.preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "        return zip(*word_pairs)\n",
        "\n",
        "    # Step 3 and Step 4\n",
        "    def tokenize(self, lang):\n",
        "        # lang = list of sentences in a language\n",
        "\n",
        "        # print(len(lang), \"example sentence: {}\".format(lang[0]))\n",
        "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
        "        lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
        "        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
        "        tensor = lang_tokenizer.texts_to_sequences(lang) \n",
        "\n",
        "        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
        "        ## and pads the sequences to match the longest sequences in the given input\n",
        "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "        return tensor, lang_tokenizer\n",
        "\n",
        "    def load_dataset(self, path, num_examples=None):\n",
        "        # creating cleaned input, output pairs\n",
        "        targ_lang, inp_lang = self.create_dataset(path, num_examples)\n",
        "\n",
        "        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)\n",
        "        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)\n",
        "\n",
        "        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "\n",
        "    def call(self, num_examples, BUFFER_SIZE, BATCH_SIZE):\n",
        "        file_path = download_nmt()\n",
        "        input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(file_path, num_examples)\n",
        "\n",
        "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
        "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
        "        val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "        return train_dataset, val_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer"
      ],
      "metadata": {
        "id": "MBtx0Pjkn35p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 32000\n",
        "BATCH_SIZE = 64\n",
        "# Let's limit the #training examples for faster training\n",
        "num_examples = 30000\n",
        "\n",
        "dataset_creator = NMTDataset('en-spa')\n",
        "train_dataset, val_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F1ddI-0n81w",
        "outputId": "44244a32-bbb3-4c41-e67a-8ad491a56281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n",
            "2654208/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlWilhLzoB6s",
        "outputId": "f4e714aa-6699-4426-b4aa-0b5c0b42ef7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 11]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "max_length_input = example_input_batch.shape[1]\n",
        "max_length_output = example_target_batch.shape[1]\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "steps_per_epoch = num_examples//BATCH_SIZE\n",
        "\n",
        "print(\"max_length_english, max_length_spanish, vocab_size_english, vocab_size_spanish\")\n",
        "max_length_input, max_length_output, vocab_inp_size, vocab_tar_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEe8o2gsoE78",
        "outputId": "3443536e-ac41-406f-d65b-02eec8ff0b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_length_english, max_length_spanish, vocab_size_english, vocab_size_spanish\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 11, 9415, 4936)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### \n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    ##-------- LSTM layer in Encoder ------- ##\n",
        "    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, h, c = self.lstm_layer(x, initial_state = hidden)\n",
        "    return output, h, c\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]"
      ],
      "metadata": {
        "id": "wq6ySTNSoIXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test Encoder Stack\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
        "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIIm1qi2oS21",
        "outputId": "617e4b6c-f9da-49e7-bd5b-cc0afda3bba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder h vecotr shape: (batch size, units) (64, 1024)\n",
            "Encoder c vector shape: (batch size, units) (64, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.attention_type = attention_type\n",
        "\n",
        "    # Embedding Layer\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    #Final Dense layer on which softmax will be applied\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # Define the fundamental cell for decoder recurrent structure\n",
        "    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
        "\n",
        "\n",
        "\n",
        "    # Sampler\n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "\n",
        "    # Create attention mechanism with memory = None\n",
        "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
        "                                                              None, self.batch_sz*[max_length_input], self.attention_type)\n",
        "\n",
        "    # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
        "    self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
        "\n",
        "    # Define the decoder with respect to fundamental rnn cell\n",
        "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
        "\n",
        "\n",
        "  def build_rnn_cell(self, batch_sz):\n",
        "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
        "                                  self.attention_mechanism, attention_layer_size=self.dec_units)\n",
        "    return rnn_cell\n",
        "\n",
        "  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n",
        "    # ------------- #\n",
        "    # typ: Which sort of attention (Bahdanau, Luong)\n",
        "    # dec_units: final dimension of attention outputs \n",
        "    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
        "    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
        "\n",
        "    if(attention_type=='bahdanau'):\n",
        "      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "    else:\n",
        "      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
        "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
        "    return decoder_initial_state\n",
        "\n",
        "\n",
        "  def call(self, inputs, initial_state):\n",
        "    x = self.embedding(inputs)\n",
        "    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_output-1])\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "qtRbHbbeoVpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test decoder stack\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, 'luong')\n",
        "sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\n",
        "decoder.attention_mechanism.setup_memory(sample_output)\n",
        "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
        "\n",
        "\n",
        "sample_decoder_outputs = decoder(sample_x, initial_state)\n",
        "\n",
        "print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF3ybgAnoaTZ",
        "outputId": "931e2130-163e-4da9-a3d4-68e1e8ba0a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Outputs Shape:  (64, 10, 4936)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  # real shape = (BATCH_SIZE, max_length_output)\n",
        "  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
        "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "  loss = cross_entropy(y_true=real, y_pred=pred)\n",
        "  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)  \n",
        "  loss = mask* loss\n",
        "  loss = tf.reduce_mean(loss)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "8yryi5QmogyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "metadata": {
        "id": "5eK-MezmokHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
        "\n",
        "\n",
        "    dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
        "    real = targ[ : , 1: ]         # ignore <start> token\n",
        "\n",
        "    # Set the AttentionMechanism object with encoder_outputs\n",
        "    decoder.attention_mechanism.setup_memory(enc_output)\n",
        "\n",
        "    # Create AttentionWrapperState as initial_state for decoder\n",
        "    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
        "    pred = decoder(dec_input, decoder_initial_state)\n",
        "    logits = pred.rnn_output\n",
        "    loss = loss_function(real, logits)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "1w0-61GKomdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B0aIOkxop_o",
        "outputId": "02c11b9d-c58b-43fb-b88e-167a4ae93a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 4.8631\n",
            "Epoch 1 Batch 100 Loss 2.3632\n",
            "Epoch 1 Batch 200 Loss 2.0653\n",
            "Epoch 1 Batch 300 Loss 1.8186\n",
            "Epoch 1 Loss 1.7401\n",
            "Time taken for 1 epoch 51.1211633682251 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.6760\n",
            "Epoch 2 Batch 100 Loss 1.6408\n",
            "Epoch 2 Batch 200 Loss 1.4777\n",
            "Epoch 2 Batch 300 Loss 1.1955\n",
            "Epoch 2 Loss 1.1957\n",
            "Time taken for 1 epoch 23.544209241867065 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.1065\n",
            "Epoch 3 Batch 100 Loss 1.2939\n",
            "Epoch 3 Batch 200 Loss 1.1179\n",
            "Epoch 3 Batch 300 Loss 0.9257\n",
            "Epoch 3 Loss 0.8741\n",
            "Time taken for 1 epoch 24.062036514282227 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.6874\n",
            "Epoch 4 Batch 100 Loss 0.8422\n",
            "Epoch 4 Batch 200 Loss 0.9022\n",
            "Epoch 4 Batch 300 Loss 0.7015\n",
            "Epoch 4 Loss 0.6049\n",
            "Time taken for 1 epoch 23.86753273010254 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.4595\n",
            "Epoch 5 Batch 100 Loss 0.4951\n",
            "Epoch 5 Batch 200 Loss 0.4989\n",
            "Epoch 5 Batch 300 Loss 0.5182\n",
            "Epoch 5 Loss 0.4210\n",
            "Time taken for 1 epoch 24.808829069137573 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.3879\n",
            "Epoch 6 Batch 100 Loss 0.3479\n",
            "Epoch 6 Batch 200 Loss 0.3645\n",
            "Epoch 6 Batch 300 Loss 0.4390\n",
            "Epoch 6 Loss 0.3031\n",
            "Time taken for 1 epoch 23.82492208480835 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.2932\n",
            "Epoch 7 Batch 100 Loss 0.2348\n",
            "Epoch 7 Batch 200 Loss 0.3122\n",
            "Epoch 7 Batch 300 Loss 0.3119\n",
            "Epoch 7 Loss 0.2320\n",
            "Time taken for 1 epoch 22.798866271972656 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.2692\n",
            "Epoch 8 Batch 100 Loss 0.2995\n",
            "Epoch 8 Batch 200 Loss 0.2275\n",
            "Epoch 8 Batch 300 Loss 0.2423\n",
            "Epoch 8 Loss 0.1861\n",
            "Time taken for 1 epoch 22.9545156955719 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1529\n",
            "Epoch 9 Batch 100 Loss 0.1267\n",
            "Epoch 9 Batch 200 Loss 0.1996\n",
            "Epoch 9 Batch 300 Loss 0.2904\n",
            "Epoch 9 Loss 0.1525\n",
            "Time taken for 1 epoch 22.312143087387085 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1569\n",
            "Epoch 10 Batch 100 Loss 0.1229\n",
            "Epoch 10 Batch 200 Loss 0.1562\n",
            "Epoch 10 Batch 300 Loss 0.2085\n",
            "Epoch 10 Loss 0.1299\n",
            "Time taken for 1 epoch 22.88535165786743 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tLgZurBIotn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_sentence(sentence):\n",
        "  sentence = dataset_creator.preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                          maxlen=max_length_input,\n",
        "                                                          padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  inference_batch_size = inputs.shape[0]\n",
        "  result = ''\n",
        "\n",
        "  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
        "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
        "\n",
        "  dec_h = enc_h\n",
        "  dec_c = enc_c\n",
        "\n",
        "  start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
        "  end_token = targ_lang.word_index['<end>']\n",
        "\n",
        "  greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "\n",
        "  # Instantiate BasicDecoder object\n",
        "  decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
        "  # Setup Memory in decoder stack\n",
        "  decoder.attention_mechanism.setup_memory(enc_out)\n",
        "\n",
        "  # set decoder_initial_state\n",
        "  decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
        "\n",
        "\n",
        "  ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
        "  ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
        "  ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
        "\n",
        "  decoder_embedding_matrix = decoder.embedding.variables[0]\n",
        "\n",
        "  outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
        "  return outputs.sample_id.numpy()\n",
        "\n",
        "def translate(sentence):\n",
        "  result = evaluate_sentence(sentence)\n",
        "  print(result)\n",
        "  result = targ_lang.sequences_to_texts(result)\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))"
      ],
      "metadata": {
        "id": "eZXpRI7XowmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IInZGJzmo1Yf",
        "outputId": "23d433cb-41a2-475d-dea1-557410d2f4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f82df0d4d10>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(u'hace mucho frio aqui.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcdhFtDUo471",
        "outputId": "fab04b69-a9ea-4b7f-e125-e3139e731ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 11  12 184  60 184  40   4   3]]\n",
            "Input: hace mucho frio aqui.\n",
            "Predicted translation: ['it s cold of cold here . <end>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(u'esta es mi vida.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2F-jORxo_9Y",
        "outputId": "a6ecfcf9-7c96-43c2-9d53-84cd45026256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 20   9  22 190   4   3]]\n",
            "Input: esta es mi vida.\n",
            "Predicted translation: ['this is my life . <end>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(u'¿todavia estan en casa?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NB7Vsl3pIzO",
        "outputId": "41e6f8ed-fed0-41d0-e939-3215fb1f7b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 25   7 104  56  90   8   3]]\n",
            "Input: ¿todavia estan en casa?\n",
            "Predicted translation: ['are you still at home ? <end>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wrong translation\n",
        "translate(u'trata de averiguarlo.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IltFfiG8pLRs",
        "outputId": "480b72a0-a4ee-40a6-d446-3f4859705dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[126  16 892  11   4   3]]\n",
            "Input: trata de averiguarlo.\n",
            "Predicted translation: ['try to figure it . <end>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_evaluate_sentence(sentence, beam_width=3):\n",
        "  sentence = dataset_creator.preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                          maxlen=max_length_input,\n",
        "                                                          padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  inference_batch_size = inputs.shape[0]\n",
        "  result = ''\n",
        "\n",
        "  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
        "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
        "\n",
        "  dec_h = enc_h\n",
        "  dec_c = enc_c\n",
        "\n",
        "  start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
        "  end_token = targ_lang.word_index['<end>']\n",
        "\n",
        "  # From official documentation\n",
        "  # NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n",
        "  # The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n",
        "  # The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n",
        "  # The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n",
        "\n",
        "  enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n",
        "  decoder.attention_mechanism.setup_memory(enc_out)\n",
        "  print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\n",
        "\n",
        "  # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
        "  hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n",
        "  decoder_initial_state = decoder.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n",
        "  decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n",
        "\n",
        "  # Instantiate BeamSearchDecoder\n",
        "  decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.rnn_cell,beam_width=beam_width, output_layer=decoder.fc)\n",
        "  decoder_embedding_matrix = decoder.embedding.variables[0]\n",
        "\n",
        "  # The BeamSearchDecoder object's call() function takes care of everything.\n",
        "  outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\n",
        "  # outputs is tfa.seq2seq.FinalBeamSearchDecoderOutput object. \n",
        "  # The final beam predictions are stored in outputs.predicted_id\n",
        "  # outputs.beam_search_decoder_output is a tfa.seq2seq.BeamSearchDecoderOutput object which keep tracks of beam_scores and parent_ids while performing a beam decoding step\n",
        "  # final_state = tfa.seq2seq.BeamSearchDecoderState object.\n",
        "  # Sequence Length = [inference_batch_size, beam_width] details the maximum length of the beams that are generated\n",
        "\n",
        "\n",
        "  # outputs.predicted_id.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
        "  # outputs.beam_search_decoder_output.scores.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
        "  # Convert the shape of outputs and beam_scores to (inference_batch_size, beam_width, time_step_outputs)\n",
        "  final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n",
        "  beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n",
        "\n",
        "  return final_outputs.numpy(), beam_scores.numpy()"
      ],
      "metadata": {
        "id": "M9LBARMwpMXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_translate(sentence):\n",
        "  result, beam_scores = beam_evaluate_sentence(sentence)\n",
        "  print(result.shape, beam_scores.shape)\n",
        "  for beam, score in zip(result, beam_scores):\n",
        "    print(beam.shape, score.shape)\n",
        "    output = targ_lang.sequences_to_texts(beam)\n",
        "    output = [a[:a.index('<end>')] for a in output]\n",
        "    beam_score = [a.sum() for a in score]\n",
        "    print('Input: %s' % (sentence))\n",
        "    for i in range(len(output)):\n",
        "      print('{} Predicted translation: {}  {}'.format(i+1, output[i], beam_score[i]))"
      ],
      "metadata": {
        "id": "eU6KBbH7pSMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beam_translate(u'hace mucho frio aqui.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI-jmSqcpVvn",
        "outputId": "625673a7-8845-4fbb-b51a-a4fb0f0d1843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 16, 1024)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/resource_loader.py:103: UserWarning: You are currently using TensorFlow 2.7.0 and trying to load a custom op (custom_ops/seq2seq/_beam_search_ops.so).\n",
            "TensorFlow Addons has compiled its custom ops against TensorFlow 2.2.0, and there are no compatibility guarantees between the two versions. \n",
            "This means that you might get segfaults when loading the custom op, or other kind of low-level errors.\n",
            " If you do, do not file an issue on Github. This is a known limitation.\n",
            "\n",
            "It might help you to fallback to pure Python ops with TF_ADDONS_PY_OPS . To do that, see https://github.com/tensorflow/addons#gpucpu-custom-ops \n",
            "\n",
            "You can also change the TensorFlow version installed on your system. You would need a TensorFlow version equal to or above 2.2.0 and strictly below 2.3.0.\n",
            " Note that nightly versions of TensorFlow, as well as non-pip TensorFlow like `conda install tensorflow` or compiled from source are not supported.\n",
            "\n",
            "The last solution is to find the TensorFlow Addons version that has custom ops compatible with the TensorFlow installed on your system. To do that, refer to the readme: https://github.com/tensorflow/addons\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/options.py:47: RuntimeWarning: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_addons/seq2seq/beam_search_decoder.py\", line 239, in gather_tree\n",
            "    return _beam_search_so.ops.addons_gather_tree(\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/resource_loader.py\", line 64, in ops\n",
            "    self._ops = tf.load_op_library(get_path_to_datafile(self.relative_path))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/load_library.py\", line 58, in load_op_library\n",
            "    lib_handle = py_tf.TF_LoadLibrary(library_filename)\n",
            "tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.7/dist-packages/tensorflow_addons/custom_ops/seq2seq/_beam_search_ops.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb\n",
            "\n",
            "\n",
            "The gather_tree C++/CUDA custom op could not be loaded.\n",
            "For this reason, Addons will fallback to an implementation written\n",
            "in Python with public TensorFlow ops. There worst you might experience with\n",
            "this is a moderate slowdown on GPU. There can be multiple\n",
            "reason for this loading error, one of them may be an ABI incompatibility between\n",
            "the TensorFlow installed on your system and the TensorFlow used to compile\n",
            "TensorFlow Addons' custom ops. The stacktrace generated when loading the\n",
            "shared object file was displayed above.\n",
            "\n",
            "If you want this warning to disappear, either make sure the TensorFlow installed\n",
            "is compatible with this version of Addons, or tell TensorFlow Addons to\n",
            "prefer using Python implementations and not custom C++/CUDA ones. You can do that\n",
            "by changing the TF_ADDONS_PY_OPS flag\n",
            "either with the environment variable:\n",
            "```bash\n",
            "TF_ADDONS_PY_OPS=1 python my_script.py\n",
            "```\n",
            "or in your code, after your imports:\n",
            "```python\n",
            "import tensorflow_addons as tfa\n",
            "import ...\n",
            "import ...\n",
            "\n",
            "tfa.options.TF_ADDONS_PY_OPS = True\n",
            "```\n",
            "\n",
            "  warnings.warn(warning_msg, RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3, 8) (1, 3, 8)\n",
            "(3, 8) (3, 8)\n",
            "Input: hace mucho frio aqui.\n",
            "1 Predicted translation: it s cold of cold here .   -7.897136688232422\n",
            "2 Predicted translation: it s cold by cold here .   -22.55335235595703\n",
            "3 Predicted translation: it s cold by here .   -25.368595123291016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beam_translate(u'¿todavia estan en casa?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOlI24wepbB_",
        "outputId": "e9413697-e44b-4e61-d70b-09d176bd743a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 16, 1024)\n",
            "(1, 3, 7) (1, 3, 7)\n",
            "(3, 7) (3, 7)\n",
            "Input: ¿todavia estan en casa?\n",
            "1 Predicted translation: are you still at home ?   -1.1845848560333252\n",
            "2 Predicted translation: aren t you at home ?   -18.388198852539062\n",
            "3 Predicted translation: are you still home ?   -26.85214614868164\n"
          ]
        }
      ]
    }
  ]
}