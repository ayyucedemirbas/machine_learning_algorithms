{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pre-trained_GloVe_Embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fupCdpJwajVq",
        "outputId": "79c739bb-0330-44fc-9b2f-8b69d284143f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-06 09:43:28--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-02-06 09:43:28--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-02-06 09:43:29--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.91MB/s    in 2m 41s  \n",
            "\n",
            "2022-02-06 09:46:11 (5.10 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjQPiPToauNx",
        "outputId": "782d8ea5-ce6a-4898-d7e7-1cdbded85dc5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "KczwBV4pb3Fn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIMENSION=50"
      ],
      "metadata": {
        "id": "NVOsNqZDb77e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_weights_file_path = os.path.join(f'glove.6B.{EMBEDDING_DIMENSION}d.txt')"
      ],
      "metadata": {
        "id": "pIU4dCrtb1j5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_weights_file_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5GgKKK-IcBYp",
        "outputId": "c151ce51-c8a2-41eb-b1b9-b842ce9a4fe8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'glove.6B.50d.txt'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "iSzfd1NpcbxC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "UOxR0jVAdnYU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index=None\n",
        "file = open(glove_weights_file_path)\n",
        "tokenizer = Tokenizer(num_words=40000, lower= 1, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(file)\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "7OoU8LradyPp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights=[]\n",
        "file = open(glove_weights_file_path)\n",
        "for index, line in enumerate(file):\n",
        "    values = line.split() # Word and weights separated by space\n",
        "    word = values[0] # Word is first symbol on each line\n",
        "    #print(word)\n",
        "    word_weights = np.asarray(values[1:], dtype=np.float32) # Remainder of line is weights for word\n",
        "    #print(word_weights)\n",
        "    word_index[word] = index + 1 # PAD is our zeroth index so shift by one\n",
        "    #print(word_index[word])\n",
        "    weights.append(word_weights)\n",
        "\n",
        "    if index + 1 == 40_000:\n",
        "        # Limit vocabulary to top 40k terms\n",
        "        print('break')\n",
        "        break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zJP0LlmcD90",
        "outputId": "8d8fe289-e37c-4416-d7a8-9660d22f7192"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "break\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(weights[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSRmO6MbfaQ_",
        "outputId": "a5a8bf34-10dc-4831-dcbf-808d6e3a0814"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIMENSION = len(weights[0])\n",
        "# Insert the PAD weights at index 0 now we know the embedding dimension\n",
        "weights.insert(0, np.random.randn(EMBEDDING_DIMENSION))\n",
        "\n",
        "# Append unknown and pad to end of vocab and initialize as random\n",
        "UNKNOWN_TOKEN=len(weights)\n",
        "word_index['UNK'] = UNKNOWN_TOKEN\n",
        "weights.append(np.random.randn(EMBEDDING_DIMENSION))\n",
        "\n",
        "# Construct our final vocab\n",
        "weights = np.asarray(weights, dtype=np.float32)\n",
        "\n",
        "VOCAB_SIZE=weights.shape[0]"
      ],
      "metadata": {
        "id": "xPTEsZjifZI0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "Gou6QoJug3EE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASVHpQqRg8fd",
        "outputId": "3ebd7cb0-e1ef-4cb2-b79c-d3551a9c1c1b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = {}\n",
        "features['word_indices'] = nltk.word_tokenize('hello world') # ['hello', 'world']\n",
        "features['word_indices'] = [word_index.get(word, UNKNOWN_TOKEN) for word in features['word_indices']]"
      ],
      "metadata": {
        "id": "4TbtFs4zgstc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_LENGTH=40002 # +2 unknown tokens"
      ],
      "metadata": {
        "id": "_L6rE1VdhTPT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_weights_initializer = tf.constant_initializer(weights)\n",
        "embedding_weights = tf.compat.v1.get_variable(\n",
        "    name='embedding_weights',\n",
        "    shape=(VOCAB_LENGTH, EMBEDDING_DIMENSION),\n",
        "    initializer=glove_weights_initializer,\n",
        "    trainable=False)\n",
        "embedding = tf.nn.embedding_lookup(embedding_weights, features['word_indices'])"
      ],
      "metadata": {
        "id": "abgOVu0khAEH"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}