{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozMuv1KDwqYH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# Other imports\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "import matplotlib.pyplot as plt\n",
        "from imutils import paths\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Random seed fixation\n",
        "tf.random.set_seed(666)\n",
        "np.random.seed(666)\n",
        "\n",
        "# How many training images for SimCLR?\n",
        "train_images = list(paths.list_images(\"./imgs/polypsset2k\"))\n",
        "print(len(train_images))\n",
        "\n",
        "# Augmentation utilities (differs from the original implementation)\n",
        "# Referred from: https://arxiv.org/pdf/2002.05709.pdf (Appendxi A\n",
        "# corresponding GitHub: https://github.com/google-research/simclr/)\n",
        "\n",
        "class CustomAugment(object):\n",
        "    def __call__(self, sample):\n",
        "        # Random flips\n",
        "        sample = self._random_apply(tf.image.flip_left_right, sample, p=0.5)\n",
        "\n",
        "        # Randomly apply transformation (color distortions) with probability p.\n",
        "        sample = self._random_apply(self._color_jitter, sample, p=0.8)\n",
        "        sample = self._random_apply(self._color_drop, sample, p=0.2)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def _color_jitter(self, x, s=1):\n",
        "        # one can also shuffle the order of following augmentations\n",
        "        # each time they are applied.\n",
        "        x = tf.image.random_brightness(x, max_delta=0.8*s)\n",
        "        x = tf.image.random_contrast(x, lower=1-0.8*s, upper=1+0.8*s)\n",
        "        x = tf.image.random_saturation(x, lower=1-0.8*s, upper=1+0.8*s)\n",
        "        x = tf.image.random_hue(x, max_delta=0.2*s)\n",
        "        x = tf.clip_by_value(x, 0, 1)\n",
        "        return x\n",
        "\n",
        "    def _color_drop(self, x):\n",
        "        x = tf.image.rgb_to_grayscale(x)\n",
        "        x = tf.tile(x, [1, 1, 1, 3])\n",
        "        return x\n",
        "\n",
        "    def _random_apply(self, func, x, p):\n",
        "        return tf.cond(\n",
        "          tf.less(tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
        "                  tf.cast(p, tf.float32)),\n",
        "          lambda: func(x),\n",
        "          lambda: x)\n",
        "\n",
        "# Build the augmentation pipeline\n",
        "data_augmentation = Sequential([Lambda(CustomAugment())])\n",
        "\n",
        "# Image preprocessing utils\n",
        "@tf.function\n",
        "def parse_images(image_path):\n",
        "    image_string = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, size=[224, 224])\n",
        "\n",
        "    return image\n",
        "\n",
        "# Create TensorFlow dataset\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(train_images)\n",
        "train_ds = (\n",
        "    train_ds\n",
        "    .map(parse_images, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    .shuffle(1024)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "base_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "len(base_model.layers)\n",
        "\n",
        "base_model.summary()\n",
        "\n",
        "last3 = to keep last three convolutional layers\n",
        "\n",
        "# Architecture utils\n",
        "def get_resnet_simclr(hidden_1, hidden_2, hidden_3):\n",
        "    base_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "    base_model.trainable = True\n",
        "    fine_tune_at = -last3\n",
        "    for layer in base_model.layers[:fine_tune_at]:\n",
        "        layer.trainable =  False\n",
        "\n",
        "    inputs = Input((224, 224, 3))\n",
        "    h = base_model(inputs)\n",
        "    h = GlobalAveragePooling2D()(h)\n",
        "\n",
        "    projection_1 = Dense(hidden_1)(h)\n",
        "    projection_1 = Activation(\"relu\")(projection_1)\n",
        "    projection_2 = Dense(hidden_2)(projection_1)\n",
        "    projection_2 = Activation(\"relu\")(projection_2)\n",
        "    projection_3 = Dense(hidden_3)(projection_2)\n",
        "\n",
        "    resnet_simclr = Model(inputs, projection_3)\n",
        "\n",
        "    return resnet_simclr\n",
        "\n",
        "from losses import _dot_simililarity_dim1 as sim_func_dim1, _dot_simililarity_dim2 as sim_func_dim2\n",
        "import helpers\n",
        "\n",
        "# Mask to remove positive examples from the batch of negative samples\n",
        "negative_mask = helpers.get_negative_mask(BATCH_SIZE)\n",
        "\n",
        "@tf.function\n",
        "def train_step(xis, xjs, model, optimizer, criterion, temperature):\n",
        "    with tf.GradientTape() as tape:\n",
        "        zis = model(xis)\n",
        "        zjs = model(xjs)\n",
        "\n",
        "        # normalize projection feature vectors\n",
        "        zis = tf.math.l2_normalize(zis, axis=1)\n",
        "        zjs = tf.math.l2_normalize(zjs, axis=1)\n",
        "\n",
        "        l_pos = sim_func_dim1(zis, zjs)\n",
        "        l_pos = tf.reshape(l_pos, (BATCH_SIZE, 1))\n",
        "        l_pos /= temperature\n",
        "\n",
        "        negatives = tf.concat([zjs, zis], axis=0)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        for positives in [zis, zjs]:\n",
        "            l_neg = sim_func_dim2(positives, negatives)\n",
        "\n",
        "            labels = tf.zeros(BATCH_SIZE, dtype=tf.int32)\n",
        "\n",
        "            l_neg = tf.boolean_mask(l_neg, negative_mask)\n",
        "            l_neg = tf.reshape(l_neg, (BATCH_SIZE, -1))\n",
        "            l_neg /= temperature\n",
        "\n",
        "            logits = tf.concat([l_pos, l_neg], axis=1)\n",
        "            loss += criterion(y_pred=logits, y_true=labels)\n",
        "\n",
        "        loss = loss / (2 * BATCH_SIZE)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss\n",
        "\n",
        "def train_simclr(model, dataset, optimizer, criterion,\n",
        "                 temperature=0.1, epochs=100):\n",
        "    step_wise_loss = []\n",
        "    epoch_wise_loss = []\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for image_batch in dataset:\n",
        "            a = data_augmentation(image_batch)\n",
        "            b = data_augmentation(image_batch)\n",
        "\n",
        "            loss = train_step(a, b, model, optimizer, criterion, temperature)\n",
        "            step_wise_loss.append(loss)\n",
        "\n",
        "        epoch_wise_loss.append(np.mean(step_wise_loss))\n",
        "        # wandb.log({\"nt_xentloss\": np.mean(step_wise_loss)})\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(\"epoch: {} loss: {:.3f}\".format(epoch + 1, np.mean(step_wise_loss)))\n",
        "\n",
        "    return epoch_wise_loss, model\n",
        "\n",
        "epoch_num = 200\n",
        "\n",
        "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                          reduction=tf.keras.losses.Reduction.SUM)\n",
        "decay_steps = 1000\n",
        "lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
        "    initial_learning_rate=0.1, decay_steps=decay_steps)\n",
        "optimizer = tf.keras.optimizers.legacy.SGD(lr_decayed_fn)\n",
        "\n",
        "resnet_simclr_2 = get_resnet_simclr(2048, 256, 128)\n",
        "\n",
        "epoch_wise_loss, resnet_simclr  = train_simclr(resnet_simclr_2, train_ds, optimizer, criterion,\n",
        "                 temperature=0.1, epochs = epoch_num)\n",
        "\n",
        "with plt.xkcd():\n",
        "    plt.plot(epoch_wise_loss)\n",
        "    plt.title(\"tau = 0.1, h1 = 256, h2 = 128, h3 = 50\")\n",
        "    plt.show()\n",
        "\n",
        "import datetime\n",
        "filename = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"resnet_simclr.h5\"\n",
        "filename\n",
        "\n",
        "resnet_simclr.save_weights(filename)\n",
        "# wandb.save(filename)\n",
        "\n",
        "filename = '20231123-221443resnet_simclr.h5'\n",
        "\n",
        "resnet_simclr = get_resnet_simclr(2048, 256, 128)\n",
        "resnet_simclr.load_weights(filename)\n",
        "resnet_simclr.summary()\n",
        "\n",
        "resnet_simclr.trainable = False\n",
        "\n",
        "resnet_simclr.summary()\n",
        "\n",
        "model_f = tf.keras.Sequential()\n",
        "model_f.add(resnet_simclr)\n",
        "model_f.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model_f.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model_f.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "\n",
        "model_f.summary() #\n",
        "\n",
        "METRICS = [\n",
        "      tf.keras.metrics.TruePositives(name='tp'),\n",
        "      tf.keras.metrics.FalsePositives(name='fp'),\n",
        "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "      tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall'),\n",
        "      # tf.keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "model_f.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001),\n",
        "              loss = tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics = [METRICS]\n",
        "             )\n",
        "\n",
        "steps_per_epoch = train_count//batch_size\n",
        "val_step = val_count//batch_size\n",
        "\n",
        "earlystop_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                                                   min_delta=0, patience=5, verbose=1,\n",
        "                                                   mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "history_f = model_f.fit(train_ds, epochs = 50, steps_per_epoch=steps_per_epoch,\n",
        "                   validation_data= val_ds, validation_steps=val_step,\n",
        "                    callbacks = [earlystop_callback])\n",
        "\n",
        "model_f.save('model_f_1125a.h5')\n",
        "\n",
        "model_f.evaluate(test_ds, verbose=1)"
      ]
    }
  ]
}