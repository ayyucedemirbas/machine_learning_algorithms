{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment\n",
    "\n",
    "## Saving and loading models, with application to the EuroSat dataset\n",
    "\n",
    "### Instructions\n",
    "\n",
    "In this notebook, you will create a neural network that classifies land uses and land covers from satellite imagery. You will save your model using Tensorflow's callbacks and reload it later. You will also load in a pre-trained neural network classifier and compare performance with it. \n",
    "\n",
    "Some code cells are provided for you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "Don't move or edit this first line - this is what the automatic grader looks for to recognise graded cells. These cells require you to write your own code to complete them, and are automatically graded when you submit the notebook. Don't edit the function name or signature provided in these cells, otherwise the automatic grader might not function properly. Inside these graded cells, you can use any functions or classes that are imported below, but make sure you don't use any variables that are outside the scope of the function.\n",
    "\n",
    "### How to submit\n",
    "\n",
    "Complete all the tasks you are asked for in the worksheet. When you have finished and are happy with your code, press the **Submit Assignment** button at the top of this notebook.\n",
    "\n",
    "### Let's get started!\n",
    "\n",
    "We'll start running some imports, and loading the dataset. Do not edit the existing imports in the following cell. If you would like to make further Tensorflow imports, you should add them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# If you would like to make further imports from tensorflow, add them here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EuroSAT overview image](data/eurosat_overview.jpg)\n",
    "\n",
    "#### The EuroSAT dataset\n",
    "\n",
    "In this assignment, you will use the [EuroSAT dataset](https://github.com/phelber/EuroSAT). It consists of 27000 labelled Sentinel-2 satellite images of different land uses: residential, industrial, highway, river, forest, pasture, herbaceous vegetation, annual crop, permanent crop and sea/lake. For a reference, see the following papers:\n",
    "- Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Patrick Helber, Benjamin Bischke, Andreas Dengel, Damian Borth. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.\n",
    "- Introducing EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification. Patrick Helber, Benjamin Bischke, Andreas Dengel. 2018 IEEE International Geoscience and Remote Sensing Symposium, 2018.\n",
    "\n",
    "Your goal is to construct a neural network that classifies a satellite image into one of these 10 classes, as well as applying some of the saving and loading techniques you have learned in the previous sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data\n",
    "\n",
    "The dataset you will train your model on is a subset of the total data, with 4000 training images and 1000 testing images, with roughly equal numbers of each class. The code to import the data is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import the Eurosat data\n",
    "\n",
    "def load_eurosat_data():\n",
    "    data_dir = 'data/'\n",
    "    x_train = np.load(os.path.join(data_dir, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(data_dir, 'y_train.npy'))\n",
    "    x_test  = np.load(os.path.join(data_dir, 'x_test.npy'))\n",
    "    y_test  = np.load(os.path.join(data_dir, 'y_test.npy'))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_eurosat_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now construct a model to fit to the data. Using the Sequential API, build your model according to the following specifications:\n",
    "\n",
    "* The model should use the input_shape in the function argument to set the input size in the first layer.\n",
    "* The first layer should be a Conv2D layer with 16 filters, a 3x3 kernel size, a ReLU activation function and 'SAME' padding. Name this layer 'conv_1'.\n",
    "* The second layer should also be a Conv2D layer with 8 filters, a 3x3 kernel size, a ReLU activation function and 'SAME' padding. Name this layer 'conv_2'.\n",
    "* The third layer should be a MaxPooling2D layer with a pooling window size of 8x8. Name this layer 'pool_1'.\n",
    "* The fourth layer should be a Flatten layer, named 'flatten'.\n",
    "* The fifth layer should be a Dense layer with 32 units, a ReLU activation. Name this layer 'dense_1'.\n",
    "* The sixth and final layer should be a Dense layer with 10 units and softmax activation. Name this layer 'dense_2'.\n",
    "\n",
    "In total, the network should have 6 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_new_model(input_shape):\n",
    "    \"\"\"\n",
    "    This function should build a Sequential model according to the above specification. Ensure the \n",
    "    weights are initialised by providing the input_shape argument in the first layer, given by the\n",
    "    function argument.\n",
    "    Your function should also compile the model with the Adam optimiser, sparse categorical cross\n",
    "    entropy loss function, and a single accuracy metric.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(filters=16, input_shape=input_shape, kernel_size=(3, 3), \n",
    "               activation='relu', name='conv_1', padding='SAME'),\n",
    "        Conv2D(filters=8, kernel_size=(3, 3), activation='relu',padding='SAME', name='conv_2'),\n",
    "        MaxPooling2D(pool_size=(8, 8), name='pool_1'),\n",
    "        Flatten(name='flatten'),\n",
    "        Dense(units=32, activation='relu', name='dense_1'),\n",
    "        Dense(units=10, activation='softmax', name='dense_2')\n",
    "    ])\n",
    "     \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to create the model\n",
    "\n",
    "model = get_new_model(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to define a function to evaluate a model's test accuracy\n",
    "\n",
    "def get_test_accuracy(model, x_test, y_test):\n",
    "    \"\"\"Test model classification accuracy\"\"\"\n",
    "    test_loss, test_acc = model.evaluate(x=x_test, y=y_test, verbose=0)\n",
    "    print('accuracy: {acc:0.3f}'.format(acc=test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_1 (Conv2D)              (None, 64, 64, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 64, 64, 8)         1160      \n",
      "_________________________________________________________________\n",
      "pool_1 (MaxPooling2D)        (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 18,354\n",
      "Trainable params: 18,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "accuracy: 0.116\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary and calculate its initialised test accuracy\n",
    "\n",
    "model.summary()\n",
    "get_test_accuracy(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create checkpoints to save model during training, with a criterion\n",
    "\n",
    "You will now create three callbacks:\n",
    "- `checkpoint_every_epoch`: checkpoint that saves the model weights every epoch during training\n",
    "- `checkpoint_best_only`: checkpoint that saves only the weights with the highest validation accuracy. Use the testing data as the validation data.\n",
    "- `early_stopping`: early stopping object that ends training if the validation accuracy has not improved in 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following functions. \n",
    "# Make sure to not change the function names or arguments.\n",
    "\n",
    "def get_checkpoint_every_epoch():\n",
    "    \"\"\"\n",
    "    This function should return a ModelCheckpoint object that:\n",
    "    - saves the weights only at the end of every epoch\n",
    "    - saves into a directory called 'checkpoints_every_epoch' inside the current working directory\n",
    "    - generates filenames in that directory like 'checkpoint_XXX' where\n",
    "      XXX is the epoch number formatted to have three digits, e.g. 001, 002, 003, etc.\n",
    "    \"\"\"\n",
    "    checkpoint_path= 'checkpoints_every_epoch/checkpoint_{epoch:03d}'\n",
    "    checkpoint= ModelCheckpoint(filepath=checkpoint_path,\n",
    "                           frequency='epoch',\n",
    "                           save_weights_only=True,\n",
    "                           verbose=1)\n",
    "    return checkpoint\n",
    "    \n",
    "\n",
    "\n",
    "def get_checkpoint_best_only():\n",
    "    \"\"\"\n",
    "    This function should return a ModelCheckpoint object that:\n",
    "    - saves only the weights that generate the highest validation (testing) accuracy\n",
    "    - saves into a directory called 'checkpoints_best_only' inside the current working directory\n",
    "    - generates a file called 'checkpoints_best_only/checkpoint' \n",
    "    \"\"\"\n",
    "    checkpoint_best_path= 'checkpoints_best_only/checkpoint'\n",
    "    checkpoint_best= ModelCheckpoint(filepath=checkpoint_best_path,\n",
    "                           frequency='epoch',\n",
    "                           save_weights_only=True,\n",
    "                           monitor='val_accuracy',\n",
    "                           save_best_only=True,\n",
    "                           verbose=1)\n",
    "    return checkpoint_best\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_early_stopping():\n",
    "    \"\"\"\n",
    "    This function should return an EarlyStopping callback that stops training when\n",
    "    the validation (testing) accuracy has not improved in the last 3 epochs.\n",
    "    HINT: use the EarlyStopping callback with the correct 'monitor' and 'patience'\n",
    "    \"\"\"\n",
    "    return tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "    #return early_stopping\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to create the callbacks\n",
    "\n",
    "checkpoint_every_epoch = get_checkpoint_every_epoch()\n",
    "checkpoint_best_only = get_checkpoint_best_only()\n",
    "early_stopping = get_early_stopping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model using the callbacks\n",
    "\n",
    "Now, you will train the model using the three callbacks you created. If you created the callbacks correctly, three things should happen:\n",
    "- At the end of every epoch, the model weights are saved into a directory called `checkpoints_every_epoch`\n",
    "- At the end of every epoch, the model weights are saved into a directory called `checkpoints_best_only` **only** if those weights lead to the highest test accuracy\n",
    "- Training stops when the testing accuracy has not improved in three epochs.\n",
    "\n",
    "You should then have two directories:\n",
    "- A directory called `checkpoints_every_epoch` containing filenames that include `checkpoint_001`, `checkpoint_002`, etc with the `001`, `002` corresponding to the epoch\n",
    "- A directory called `checkpoints_best_only` containing filenames that include `checkpoint`, which contain only the weights leading to the highest testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 2.0282 - accuracy: 0.2291\n",
      "Epoch 00001: saving model to checkpoints_every_epoch/checkpoint_001\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.31900, saving model to checkpoints_best_only/checkpoint\n",
      "4000/4000 [==============================] - 75s 19ms/sample - loss: 2.0255 - accuracy: 0.2298 - val_loss: 1.6626 - val_accuracy: 0.3190\n",
      "Epoch 2/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 1.4559 - accuracy: 0.4463\n",
      "Epoch 00002: saving model to checkpoints_every_epoch/checkpoint_002\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.31900 to 0.44600, saving model to checkpoints_best_only/checkpoint\n",
      "4000/4000 [==============================] - 74s 18ms/sample - loss: 1.4544 - accuracy: 0.4465 - val_loss: 1.4097 - val_accuracy: 0.4460\n",
      "Epoch 3/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 1.3146 - accuracy: 0.4730\n",
      "Epoch 00003: saving model to checkpoints_every_epoch/checkpoint_003\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.44600 to 0.50900, saving model to checkpoints_best_only/checkpoint\n",
      "4000/4000 [==============================] - 73s 18ms/sample - loss: 1.3169 - accuracy: 0.4728 - val_loss: 1.2685 - val_accuracy: 0.5090\n",
      "Epoch 4/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 1.2223 - accuracy: 0.5295\n",
      "Epoch 00004: saving model to checkpoints_every_epoch/checkpoint_004\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.50900\n",
      "4000/4000 [==============================] - 75s 19ms/sample - loss: 1.2228 - accuracy: 0.5295 - val_loss: 1.2690 - val_accuracy: 0.5070\n",
      "Epoch 5/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 1.1337 - accuracy: 0.5748\n",
      "Epoch 00005: saving model to checkpoints_every_epoch/checkpoint_005\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.50900 to 0.56400, saving model to checkpoints_best_only/checkpoint\n",
      "4000/4000 [==============================] - 73s 18ms/sample - loss: 1.1323 - accuracy: 0.5755 - val_loss: 1.2033 - val_accuracy: 0.5640\n",
      "Epoch 6/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 1.0465 - accuracy: 0.6298\n",
      "Epoch 00006: saving model to checkpoints_every_epoch/checkpoint_006\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.56400 to 0.65300, saving model to checkpoints_best_only/checkpoint\n",
      "4000/4000 [==============================] - 74s 18ms/sample - loss: 1.0450 - accuracy: 0.6308 - val_loss: 1.0187 - val_accuracy: 0.6530\n",
      "Epoch 7/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.9688 - accuracy: 0.6482\n",
      "Epoch 00007: saving model to checkpoints_every_epoch/checkpoint_007\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.65300 to 0.65400, saving model to checkpoints_best_only/checkpoint\n",
      "4000/4000 [==============================] - 75s 19ms/sample - loss: 0.9670 - accuracy: 0.6485 - val_loss: 0.9793 - val_accuracy: 0.6540\n",
      "Epoch 8/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.8791 - accuracy: 0.6847\n",
      "Epoch 00008: saving model to checkpoints_every_epoch/checkpoint_008\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.65400\n",
      "4000/4000 [==============================] - 73s 18ms/sample - loss: 0.8810 - accuracy: 0.6840 - val_loss: 0.9677 - val_accuracy: 0.6370\n",
      "Epoch 9/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.8375 - accuracy: 0.7011\n",
      "Epoch 00009: saving model to checkpoints_every_epoch/checkpoint_009\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.65400 to 0.68200, saving model to checkpoints_best_only/checkpoint\n",
      "4000/4000 [==============================] - 73s 18ms/sample - loss: 0.8393 - accuracy: 0.7010 - val_loss: 0.9312 - val_accuracy: 0.6820\n",
      "Epoch 10/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.7961 - accuracy: 0.7152\n",
      "Epoch 00010: saving model to checkpoints_every_epoch/checkpoint_010\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.68200 to 0.69700, saving model to checkpoints_best_only/checkpoint\n",
      "4000/4000 [==============================] - 73s 18ms/sample - loss: 0.7985 - accuracy: 0.7147 - val_loss: 0.8662 - val_accuracy: 0.6970\n",
      "Epoch 11/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.7539 - accuracy: 0.7273\n",
      "Epoch 00011: saving model to checkpoints_every_epoch/checkpoint_011\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.69700\n",
      "4000/4000 [==============================] - 73s 18ms/sample - loss: 0.7532 - accuracy: 0.7272 - val_loss: 0.8948 - val_accuracy: 0.6840\n",
      "Epoch 12/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.7265 - accuracy: 0.7467\n",
      "Epoch 00012: saving model to checkpoints_every_epoch/checkpoint_012\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.69700 to 0.72600, saving model to checkpoints_best_only/checkpoint\n",
      "4000/4000 [==============================] - 77s 19ms/sample - loss: 0.7247 - accuracy: 0.7480 - val_loss: 0.8161 - val_accuracy: 0.7260\n",
      "Epoch 13/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.7124 - accuracy: 0.7424\n",
      "Epoch 00013: saving model to checkpoints_every_epoch/checkpoint_013\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.72600\n",
      "4000/4000 [==============================] - 76s 19ms/sample - loss: 0.7121 - accuracy: 0.7423 - val_loss: 0.8113 - val_accuracy: 0.7170\n",
      "Epoch 14/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.6753 - accuracy: 0.7553\n",
      "Epoch 00014: saving model to checkpoints_every_epoch/checkpoint_014\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.72600\n",
      "4000/4000 [==============================] - 76s 19ms/sample - loss: 0.6761 - accuracy: 0.7550 - val_loss: 0.7946 - val_accuracy: 0.7200\n",
      "Epoch 15/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.6582 - accuracy: 0.7659\n",
      "Epoch 00015: saving model to checkpoints_every_epoch/checkpoint_015\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.72600\n",
      "4000/4000 [==============================] - 77s 19ms/sample - loss: 0.6584 - accuracy: 0.7657 - val_loss: 0.8280 - val_accuracy: 0.7090\n",
      "Epoch 16/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.6453 - accuracy: 0.7709\n",
      "Epoch 00016: saving model to checkpoints_every_epoch/checkpoint_016\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.72600\n",
      "4000/4000 [==============================] - 74s 19ms/sample - loss: 0.6470 - accuracy: 0.7705 - val_loss: 0.8281 - val_accuracy: 0.7160\n",
      "Epoch 17/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.6365 - accuracy: 0.7732\n",
      "Epoch 00017: saving model to checkpoints_every_epoch/checkpoint_017\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.72600\n",
      "4000/4000 [==============================] - 74s 18ms/sample - loss: 0.6383 - accuracy: 0.7725 - val_loss: 0.8283 - val_accuracy: 0.6960\n",
      "Epoch 18/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.6086 - accuracy: 0.7828\n",
      "Epoch 00018: saving model to checkpoints_every_epoch/checkpoint_018\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.72600\n",
      "4000/4000 [==============================] - 74s 18ms/sample - loss: 0.6068 - accuracy: 0.7830 - val_loss: 0.8380 - val_accuracy: 0.7060\n",
      "Epoch 19/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.6049 - accuracy: 0.7873\n",
      "Epoch 00019: saving model to checkpoints_every_epoch/checkpoint_019\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.72600 to 0.73200, saving model to checkpoints_best_only/checkpoint\n",
      "4000/4000 [==============================] - 73s 18ms/sample - loss: 0.6083 - accuracy: 0.7862 - val_loss: 0.7495 - val_accuracy: 0.7320\n",
      "Epoch 20/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.5736 - accuracy: 0.7999\n",
      "Epoch 00020: saving model to checkpoints_every_epoch/checkpoint_020\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.73200\n",
      "4000/4000 [==============================] - 73s 18ms/sample - loss: 0.5729 - accuracy: 0.8002 - val_loss: 0.7690 - val_accuracy: 0.7210\n",
      "Epoch 21/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.5683 - accuracy: 0.7994\n",
      "Epoch 00021: saving model to checkpoints_every_epoch/checkpoint_021\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.73200\n",
      "4000/4000 [==============================] - 74s 18ms/sample - loss: 0.5688 - accuracy: 0.7985 - val_loss: 0.7464 - val_accuracy: 0.7200\n",
      "Epoch 22/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.5654 - accuracy: 0.8029\n",
      "Epoch 00022: saving model to checkpoints_every_epoch/checkpoint_022\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.73200\n",
      "4000/4000 [==============================] - 74s 18ms/sample - loss: 0.5640 - accuracy: 0.8033 - val_loss: 0.7887 - val_accuracy: 0.7140\n",
      "Epoch 23/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7981\n",
      "Epoch 00023: saving model to checkpoints_every_epoch/checkpoint_023\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.73200\n",
      "4000/4000 [==============================] - 72s 18ms/sample - loss: 0.5643 - accuracy: 0.7980 - val_loss: 0.8272 - val_accuracy: 0.7150\n",
      "Epoch 24/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.5395 - accuracy: 0.8077\n",
      "Epoch 00024: saving model to checkpoints_every_epoch/checkpoint_024\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.73200\n",
      "4000/4000 [==============================] - 72s 18ms/sample - loss: 0.5400 - accuracy: 0.8080 - val_loss: 0.7458 - val_accuracy: 0.7320\n",
      "Epoch 25/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.5185 - accuracy: 0.8175\n",
      "Epoch 00025: saving model to checkpoints_every_epoch/checkpoint_025\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.73200\n",
      "4000/4000 [==============================] - 73s 18ms/sample - loss: 0.5212 - accuracy: 0.8167 - val_loss: 0.7540 - val_accuracy: 0.7310\n",
      "Epoch 26/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.5320 - accuracy: 0.8122\n",
      "Epoch 00026: saving model to checkpoints_every_epoch/checkpoint_026\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.73200\n",
      "4000/4000 [==============================] - 73s 18ms/sample - loss: 0.5300 - accuracy: 0.8130 - val_loss: 0.7815 - val_accuracy: 0.7260\n",
      "Epoch 27/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.5211 - accuracy: 0.8138\n",
      "Epoch 00027: saving model to checkpoints_every_epoch/checkpoint_027\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.73200\n",
      "4000/4000 [==============================] - 73s 18ms/sample - loss: 0.5221 - accuracy: 0.8133 - val_loss: 0.7584 - val_accuracy: 0.7290\n",
      "Epoch 28/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.5175 - accuracy: 0.8168\n",
      "Epoch 00028: saving model to checkpoints_every_epoch/checkpoint_028\n",
      "\n",
      "Epoch 00028: val_accuracy improved from 0.73200 to 0.73700, saving model to checkpoints_best_only/checkpoint\n",
      "4000/4000 [==============================] - 73s 18ms/sample - loss: 0.5183 - accuracy: 0.8163 - val_loss: 0.7850 - val_accuracy: 0.7370\n",
      "Epoch 29/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4989 - accuracy: 0.8241\n",
      "Epoch 00029: saving model to checkpoints_every_epoch/checkpoint_029\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.73700\n",
      "4000/4000 [==============================] - 73s 18ms/sample - loss: 0.4991 - accuracy: 0.8238 - val_loss: 0.9206 - val_accuracy: 0.6810\n",
      "Epoch 30/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4944 - accuracy: 0.8261\n",
      "Epoch 00030: saving model to checkpoints_every_epoch/checkpoint_030\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.73700\n",
      "4000/4000 [==============================] - 73s 18ms/sample - loss: 0.4937 - accuracy: 0.8260 - val_loss: 0.7636 - val_accuracy: 0.7230\n",
      "Epoch 31/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4904 - accuracy: 0.8246\n",
      "Epoch 00031: saving model to checkpoints_every_epoch/checkpoint_031\n",
      "\n",
      "Epoch 00031: val_accuracy improved from 0.73700 to 0.74700, saving model to checkpoints_best_only/checkpoint\n",
      "4000/4000 [==============================] - 73s 18ms/sample - loss: 0.4894 - accuracy: 0.8253 - val_loss: 0.7311 - val_accuracy: 0.7470\n",
      "Epoch 32/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4678 - accuracy: 0.8349\n",
      "Epoch 00032: saving model to checkpoints_every_epoch/checkpoint_032\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.74700\n",
      "4000/4000 [==============================] - 78s 19ms/sample - loss: 0.4673 - accuracy: 0.8347 - val_loss: 0.7474 - val_accuracy: 0.7320\n",
      "Epoch 33/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4573 - accuracy: 0.8412\n",
      "Epoch 00033: saving model to checkpoints_every_epoch/checkpoint_033\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.74700\n",
      "4000/4000 [==============================] - 78s 19ms/sample - loss: 0.4567 - accuracy: 0.8415 - val_loss: 0.7256 - val_accuracy: 0.7420\n",
      "Epoch 34/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4847 - accuracy: 0.8342\n",
      "Epoch 00034: saving model to checkpoints_every_epoch/checkpoint_034\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.74700\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.4863 - accuracy: 0.8335 - val_loss: 0.7726 - val_accuracy: 0.7410\n",
      "Epoch 35/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4522 - accuracy: 0.8412\n",
      "Epoch 00035: saving model to checkpoints_every_epoch/checkpoint_035\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.74700\n",
      "4000/4000 [==============================] - 82s 21ms/sample - loss: 0.4520 - accuracy: 0.8415 - val_loss: 0.7618 - val_accuracy: 0.7390\n",
      "Epoch 36/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4466 - accuracy: 0.8405\n",
      "Epoch 00036: saving model to checkpoints_every_epoch/checkpoint_036\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.74700\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.4475 - accuracy: 0.8405 - val_loss: 0.8049 - val_accuracy: 0.7260\n",
      "Epoch 37/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4335 - accuracy: 0.8531\n",
      "Epoch 00037: saving model to checkpoints_every_epoch/checkpoint_037\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.74700\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.4321 - accuracy: 0.8535 - val_loss: 0.7668 - val_accuracy: 0.7380\n",
      "Epoch 38/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4422 - accuracy: 0.8397\n",
      "Epoch 00038: saving model to checkpoints_every_epoch/checkpoint_038\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.74700\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.4427 - accuracy: 0.8403 - val_loss: 0.7677 - val_accuracy: 0.7380\n",
      "Epoch 39/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4195 - accuracy: 0.8543\n",
      "Epoch 00039: saving model to checkpoints_every_epoch/checkpoint_039\n",
      "\n",
      "Epoch 00039: val_accuracy improved from 0.74700 to 0.74900, saving model to checkpoints_best_only/checkpoint\n",
      "4000/4000 [==============================] - 80s 20ms/sample - loss: 0.4206 - accuracy: 0.8535 - val_loss: 0.7311 - val_accuracy: 0.7490\n",
      "Epoch 40/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4047 - accuracy: 0.8599\n",
      "Epoch 00040: saving model to checkpoints_every_epoch/checkpoint_040\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.74900\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.4069 - accuracy: 0.8590 - val_loss: 0.7751 - val_accuracy: 0.7430\n",
      "Epoch 41/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.8574\n",
      "Epoch 00041: saving model to checkpoints_every_epoch/checkpoint_041\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.74900\n",
      "4000/4000 [==============================] - 83s 21ms/sample - loss: 0.4212 - accuracy: 0.8575 - val_loss: 0.8333 - val_accuracy: 0.7420\n",
      "Epoch 42/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4176 - accuracy: 0.8541\n",
      "Epoch 00042: saving model to checkpoints_every_epoch/checkpoint_042\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.74900\n",
      "4000/4000 [==============================] - 84s 21ms/sample - loss: 0.4180 - accuracy: 0.8540 - val_loss: 0.7531 - val_accuracy: 0.7480\n",
      "Epoch 43/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.3942 - accuracy: 0.8639\n",
      "Epoch 00043: saving model to checkpoints_every_epoch/checkpoint_043\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.74900\n",
      "4000/4000 [==============================] - 83s 21ms/sample - loss: 0.3949 - accuracy: 0.8637 - val_loss: 0.7782 - val_accuracy: 0.7320\n",
      "Epoch 44/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.3954 - accuracy: 0.8632\n",
      "Epoch 00044: saving model to checkpoints_every_epoch/checkpoint_044\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.74900\n",
      "4000/4000 [==============================] - 82s 21ms/sample - loss: 0.3963 - accuracy: 0.8630 - val_loss: 0.8880 - val_accuracy: 0.7260\n",
      "Epoch 45/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4054 - accuracy: 0.8569\n",
      "Epoch 00045: saving model to checkpoints_every_epoch/checkpoint_045\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.74900\n",
      "4000/4000 [==============================] - 84s 21ms/sample - loss: 0.4061 - accuracy: 0.8568 - val_loss: 0.7640 - val_accuracy: 0.7470\n",
      "Epoch 46/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.4026 - accuracy: 0.8589\n",
      "Epoch 00046: saving model to checkpoints_every_epoch/checkpoint_046\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.74900\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 0.4029 - accuracy: 0.8587 - val_loss: 0.7882 - val_accuracy: 0.7370\n",
      "Epoch 47/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.3658 - accuracy: 0.8740\n",
      "Epoch 00047: saving model to checkpoints_every_epoch/checkpoint_047\n",
      "\n",
      "Epoch 00047: val_accuracy improved from 0.74900 to 0.75600, saving model to checkpoints_best_only/checkpoint\n",
      "4000/4000 [==============================] - 80s 20ms/sample - loss: 0.3662 - accuracy: 0.8740 - val_loss: 0.7604 - val_accuracy: 0.7560\n",
      "Epoch 48/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.3652 - accuracy: 0.8705\n",
      "Epoch 00048: saving model to checkpoints_every_epoch/checkpoint_048\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.75600\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.3664 - accuracy: 0.8698 - val_loss: 0.8707 - val_accuracy: 0.7260\n",
      "Epoch 49/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.3673 - accuracy: 0.8732\n",
      "Epoch 00049: saving model to checkpoints_every_epoch/checkpoint_049\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.75600\n",
      "4000/4000 [==============================] - 80s 20ms/sample - loss: 0.3660 - accuracy: 0.8737 - val_loss: 0.7662 - val_accuracy: 0.7550\n",
      "Epoch 50/50\n",
      "3968/4000 [============================>.] - ETA: 0s - loss: 0.3694 - accuracy: 0.8710\n",
      "Epoch 00050: saving model to checkpoints_every_epoch/checkpoint_050\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.75600\n",
      "4000/4000 [==============================] - 80s 20ms/sample - loss: 0.3682 - accuracy: 0.8715 - val_loss: 0.8251 - val_accuracy: 0.7440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f907c4e27b8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model using the callbacks you just created\n",
    "\n",
    "callbacks = [checkpoint_every_epoch, checkpoint_best_only, early_stopping]\n",
    "model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new instance of model and load on both sets of weights\n",
    "\n",
    "Now you will use the weights you just saved in a fresh model. You should create two functions, both of which take a freshly instantiated model instance:\n",
    "- `model_last_epoch` should contain the weights from the latest saved epoch\n",
    "- `model_best_epoch` should contain the weights from the saved epoch with the highest testing accuracy\n",
    "\n",
    "_Hint: use the_ `tf.train.latest_checkpoint` _function to get the filename of the latest saved checkpoint file. Check the docs_ [_here_](https://www.tensorflow.org/api_docs/python/tf/train/latest_checkpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following functions. \n",
    "# Make sure to not change the function name or arguments.\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def get_model_last_epoch(model):\n",
    "    \"\"\"\n",
    "    This function should create a new instance of the CNN you created earlier,\n",
    "    load on the weights from the last training epoch, and return this model.\n",
    "    \"\"\"\n",
    "    new_model= model\n",
    "    path=tf.train.latest_checkpoint('checkpoints_every_epoch')\n",
    "    new_model.load_weights(path)\n",
    "    \n",
    "    return new_model\n",
    "    \n",
    "def get_model_best_epoch(model):\n",
    "    \"\"\"\n",
    "    This function should create a new instance of the CNN you created earlier, load \n",
    "    on the weights leading to the highest validation accuracy, and return this model.\n",
    "    \"\"\"\n",
    "    new_model= model\n",
    "    path=tf.train.latest_checkpoint('checkpoints_best_only')\n",
    "    new_model.load_weights(path)\n",
    "    \n",
    "  \n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with last epoch weights:\n",
      "accuracy: 0.744\n",
      "\n",
      "Model with best epoch weights:\n",
      "accuracy: 0.756\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to create two models: one with the weights from the last training\n",
    "# epoch, and one with the weights leading to the highest validation (testing) accuracy.\n",
    "# Verify that the second has a higher validation (testing) accuarcy.\n",
    "\n",
    "model_last_epoch = get_model_last_epoch(get_new_model(x_train[0].shape))\n",
    "model_best_epoch = get_model_best_epoch(get_new_model(x_train[0].shape))\n",
    "print('Model with last epoch weights:')\n",
    "get_test_accuracy(model_last_epoch, x_test, y_test)\n",
    "print('')\n",
    "print('Model with best epoch weights:')\n",
    "get_test_accuracy(model_best_epoch, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load, from scratch, a model trained on the EuroSat dataset.\n",
    "\n",
    "In your workspace, you will find another model trained on the `EuroSAT` dataset in `.h5` format. This model is trained on a larger subset of the EuroSAT dataset and has a more complex architecture. The path to the model is `models/EuroSatNet.h5`. See how its testing accuracy compares to your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following functions. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_model_eurosatnet():\n",
    "    \"\"\"\n",
    "    This function should return the pretrained EuroSatNet.h5 model.\n",
    "    \"\"\"\n",
    "    model= load_model('models/EuroSatNet.h5')\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_1 (Conv2D)              (None, 64, 64, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 64, 64, 16)        6416      \n",
      "_________________________________________________________________\n",
      "pool_1 (MaxPooling2D)        (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv_3 (Conv2D)              (None, 32, 32, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv_4 (Conv2D)              (None, 32, 32, 16)        6416      \n",
      "_________________________________________________________________\n",
      "pool_2 (MaxPooling2D)        (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv_5 (Conv2D)              (None, 16, 16, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv_6 (Conv2D)              (None, 16, 16, 16)        6416      \n",
      "_________________________________________________________________\n",
      "pool_3 (MaxPooling2D)        (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv_7 (Conv2D)              (None, 8, 8, 16)          2320      \n",
      "_________________________________________________________________\n",
      "conv_8 (Conv2D)              (None, 8, 8, 16)          6416      \n",
      "_________________________________________________________________\n",
      "pool_4 (MaxPooling2D)        (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 41,626\n",
      "Trainable params: 41,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "accuracy: 0.810\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to print a summary of the EuroSatNet model, along with its validation accuracy.\n",
    "\n",
    "model_eurosatnet = get_model_eurosatnet()\n",
    "model_eurosatnet.summary()\n",
    "get_test_accuracy(model_eurosatnet, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations for completing this programming assignment! You're now ready to move on to the capstone project for this course."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "tensor-flow-2-1",
   "graded_item_id": "JaRY0",
   "launcher_item_id": "mJ8fg"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
